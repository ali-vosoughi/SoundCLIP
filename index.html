<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SoundCLIP: Can Sound Replace Vision in LLaVA With Token Substitution?</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Georgia, serif;
            line-height: 1.6;
            color: #333;
            background: #ffffff;
        }

        .container {
            max-width: 1100px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: #f8f9fa;
            padding: 20px 0;
            border-bottom: 1px solid #e9ecef;
        }

        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .logo {
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
        }

        .nav-links {
            display: flex;
            list-style: none;
            gap: 25px;
        }

        .nav-links a {
            text-decoration: none;
            color: #2c3e50;
            font-weight: 500;
        }

        .nav-links a:hover {
            color: #3498db;
            text-decoration: underline;
        }

        .title-section {
            text-align: center;
            padding: 50px 0;
        }

        .title-section h1 {
            font-size: 2.2rem;
            margin-bottom: 20px;
            color: #2c3e50;
            font-weight: normal;
        }

        .subtitle {
            font-size: 1.1rem;
            color: #6c757d;
            margin-bottom: 30px;
            font-style: italic;
        }

        .authors {
            margin: 30px 0;
        }

        .author-list {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 25px;
            margin-bottom: 15px;
        }

        .author {
            color: #2c3e50;
            font-weight: 500;
            text-decoration: none;
        }

        .author:hover {
            color: #3498db;
            text-decoration: underline;
        }

        .affiliation {
            font-size: 0.95rem;
            color: #6c757d;
            margin-top: 10px;
        }

        .btn-group {
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
            margin: 30px 0;
        }

        .btn {
            padding: 10px 20px;
            border: 1px solid #3498db;
            background: #ffffff;
            color: #3498db;
            text-decoration: none;
            font-weight: 500;
            font-size: 14px;
        }

        .btn:hover {
            background: #3498db;
            color: #ffffff;
        }

        .btn-primary {
            background: #3498db;
            color: #ffffff;
        }

        .btn-primary:hover {
            background: #2980b9;
            border-color: #2980b9;
        }

        .section {
            padding: 40px 0;
            border-bottom: 1px solid #e9ecef;
        }

        .section:last-child {
            border-bottom: none;
        }

        .section h2 {
            font-size: 1.8rem;
            margin-bottom: 25px;
            color: #2c3e50;
        }

        .section p {
            margin-bottom: 20px;
            text-align: justify;
            color: #444;
        }

        /* Demo Section Specific Styles */
        .demo-container {
            background: #f8f9fa;
            padding: 30px;
            margin: 30px 0;
            border-radius: 10px;
            border: 1px solid #e9ecef;
        }

        .demo-example {
            background: #ffffff;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 40px;
        }

        .demo-example:last-child {
            margin-bottom: 0;
        }

        .example-header {
            text-align: center;
            margin-bottom: 25px;
        }

        .example-title {
            font-size: 1.2rem;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .video-container {
            text-align: center;
            margin-bottom: 25px;
        }

        .demo-video {
            max-width: 100%;
            width: 400px;
            height: 225px;
            border: 1px solid #dee2e6;
            border-radius: 5px;
        }

        .encoder-grid {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 10px;
            margin-bottom: 25px;
        }

        .encoder-column {
            text-align: center;
        }

        .encoder-label {
            font-weight: bold;
            color: #2c3e50;
            padding: 8px;
            background: #e9ecef;
            font-size: 0.9rem;
            margin-bottom: 5px;
        }

        .mode-button {
            display: block;
            width: 100%;
            padding: 8px 5px;
            margin-bottom: 5px;
            border: 1px solid #6c757d;
            background: #ffffff;
            color: #6c757d;
            font-size: 0.8rem;
            cursor: pointer;
            text-align: center;
        }

        .mode-button:hover {
            background: #f8f9fa;
        }

        .mode-button.active {
            background: #3498db;
            color: #ffffff;
            border-color: #3498db;
        }

        .mode-button.raw {
            border-color: #e74c3c;
            color: #e74c3c;
        }

        .mode-button.raw.active {
            background: #e74c3c;
            color: #ffffff;
        }

        .caption-display {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 5px;
            padding: 20px;
            min-height: 100px;
            font-style: italic;
            color: #495057;
        }

        .caption-display.empty {
            color: #6c757d;
            text-align: center;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .current-selection {
            text-align: center;
            margin-bottom: 15px;
            font-weight: bold;
            color: #2c3e50;
        }

        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 20px;
            margin: 30px 0;
            text-align: center;
        }

        .stat-item {
            padding: 20px;
            background: #f8f9fa;
            border: 1px solid #e9ecef;
        }

        .stat-number {
            font-size: 2rem;
            font-weight: bold;
            color: #3498db;
            display: block;
        }

        .stat-label {
            font-size: 0.9rem;
            color: #6c757d;
            margin-top: 5px;
        }

        .citation-box {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            padding: 25px;
            margin: 30px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.85rem;
            color: #495057;
        }

        footer {
            background: #2c3e50;
            color: #ffffff;
            padding: 40px 0;
            margin-top: 50px;
        }

        .footer-content {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 30px;
            margin-bottom: 20px;
        }

        .footer-section h4 {
            color: #ecf0f1;
            margin-bottom: 15px;
            font-size: 1.1rem;
        }

        .footer-section a {
            color: #bdc3c7;
            text-decoration: none;
        }

        .footer-section a:hover {
            color: #3498db;
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .title-section h1 {
                font-size: 1.6rem;
            }
            
            .author-list {
                flex-direction: column;
                align-items: center;
                gap: 15px;
            }
            
            .btn-group {
                flex-direction: column;
                align-items: center;
            }
            
            .nav-links {
                display: none;
            }
            
            .encoder-grid {
                grid-template-columns: repeat(2, 1fr);
                gap: 15px;
            }
            
            .encoder-label {
                font-size: 0.8rem;
                padding: 6px;
            }
            
            .mode-button {
                font-size: 0.7rem;
                padding: 6px 3px;
            }
            
            .demo-video {
                width: 100%;
                max-width: 350px;
                height: auto;
            }
        }
    </style>
</head>
<body>
    <header>
        <nav class="container">
            <div class="logo">SoundCLIP</div>
            <ul class="nav-links">
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#demo">Demo</a></li>
                <li><a href="#dataset">Dataset</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#code">Code</a></li>
            </ul>
        </nav>
    </header>

    <div class="container">
        <section class="title-section">
            <h1>Can Sound Replace Vision in LLaVA With Token Substitution?</h1>
            <div class="subtitle">A Systematic Investigation of Audio-Visual Alignment Trade-offs in Multimodal Systems</div>
            
            <div class="authors">
                <div class="author-list">
                    <a href="https://alivosoughi.com/" class="author">Ali Vosoughi</a>
                    <a href="https://jing.vision/" class="author">Jing Bi</a>
                    <a href="https://andypinxinliu.github.io/" class="author">Pinxin Liu</a>
                    <a href="https://yunlong10.github.io/" class="author">Yunlong Tang</a>
                    <a href="https://www.cs.rochester.edu/~cxu22/index.html" class="author">Chenliang Xu</a>
                </div>
                <div class="affiliation">
                    Computer Science Department, University of Rochester, NY, USA
                </div>
            </div>
            
            <div class="btn-group">
                <a href="https://arxiv.org/abs/2506.10416" class="btn btn-primary">Paper</a>
                <a href="https://github.com/ali-vosoughi/SoundCLIP" class="btn">Code</a>
                <a href="#demo" class="btn">Interactive Demo</a>
                <a href="#citation" class="btn">Citation</a>
            </div>
        </section>

        <section id="abstract" class="section">
            <h2>Abstract</h2>
            <p>What happens when we push audio-visual alignment to its absolute limits? To systematically investigate this question, we needed datasets with granular alignment quality annotations, but existing datasets treat alignment as binary, either synchronized or not. To address this limitation, we developed a comprehensive dataset featuring detailed alignment scores that reveal the hidden spectrum of audio-visual perceptual correspondence. Using these precise scores, we create "superaligned" representations by training exclusively on the most perfectly matched audio-visual pairs, then conduct our systematic investigation into how this extreme alignment transforms perceptual model behavior across retrieval and generation tasks.</p>

            <p>Our findings reveal that the initial architectural type of the encoder determines how it responds to the alignment process. Image-centric encoders demonstrate exceptional performance in cross-modal retrieval, but this intensive alignment causes compression of unique linguistic information and reduces the quality of their text description generation. In contrast, text-centric encoders maintain better balance between the two objectives, revealing a fundamental trade-off where excessive alignment with the visual manifold leads to improved retrieval capabilities, but simultaneously reduces the richness of acoustic and linguistic information necessary for quality text description generation.</p>
        </section>

        <section id="demo" class="section">
            <h2>Interactive Demo</h2>
            <p>Explore how different audio encoders (Raw vs Projected) generate different captions for the same audiovisual content. Click on any encoder mode button to see the generated caption.</p>
            
            <div class="demo-container">
                <!-- Example 1 -->
                <div class="demo-example">
                    <div class="example-header">
                        <div class="example-title">Example 1: Guitar Playing</div>
                    </div>
                    
                    <div class="video-container">
                        <video class="demo-video" controls preload="metadata">
                            <source src="demos/example1.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    
                    <div class="encoder-grid">
                        <div class="encoder-column">
                            <div class="encoder-label">ImageBind</div>
                            <button class="mode-button" data-example="1" data-encoder="imagebind" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="1" data-encoder="imagebind" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">AudioCLIP</div>
                            <button class="mode-button" data-example="1" data-encoder="audioclip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="1" data-encoder="audioclip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">CLAP</div>
                            <button class="mode-button" data-example="1" data-encoder="clap" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="1" data-encoder="clap" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Wav2CLIP</div>
                            <button class="mode-button" data-example="1" data-encoder="wav2clip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="1" data-encoder="wav2clip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Whisper</div>
                            <button class="mode-button" data-example="1" data-encoder="whisper" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="1" data-encoder="whisper" data-mode="raw">Raw</button>
                        </div>
                    </div>
                    
                    <div class="current-selection" id="selection-1">Select an encoder mode to view generated caption</div>
                    <div class="caption-display empty" id="caption-1">Click any button above to see the generated caption for this audiovisual example.</div>
                </div>

                <!-- Example 2 -->
                <div class="demo-example">
                    <div class="example-header">
                        <div class="example-title">Example 2: Nature Sounds</div>
                    </div>
                    
                    <div class="video-container">
                        <video class="demo-video" controls preload="metadata">
                            <source src="demos/example2.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    
                    <div class="encoder-grid">
                        <div class="encoder-column">
                            <div class="encoder-label">ImageBind</div>
                            <button class="mode-button" data-example="2" data-encoder="imagebind" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="2" data-encoder="imagebind" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">AudioCLIP</div>
                            <button class="mode-button" data-example="2" data-encoder="audioclip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="2" data-encoder="audioclip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">CLAP</div>
                            <button class="mode-button" data-example="2" data-encoder="clap" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="2" data-encoder="clap" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Wav2CLIP</div>
                            <button class="mode-button" data-example="2" data-encoder="wav2clip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="2" data-encoder="wav2clip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Whisper</div>
                            <button class="mode-button" data-example="2" data-encoder="whisper" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="2" data-encoder="whisper" data-mode="raw">Raw</button>
                        </div>
                    </div>
                    
                    <div class="current-selection" id="selection-2">Select an encoder mode to view generated caption</div>
                    <div class="caption-display empty" id="caption-2">Click any button above to see the generated caption for this audiovisual example.</div>
                </div>

                <!-- Example 3 -->
                <div class="demo-example">
                    <div class="example-header">
                        <div class="example-title">Example 3: Urban Environment</div>
                    </div>
                    
                    <div class="video-container">
                        <video class="demo-video" controls preload="metadata">
                            <source src="demos/example3.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    
                    <div class="encoder-grid">
                        <div class="encoder-column">
                            <div class="encoder-label">ImageBind</div>
                            <button class="mode-button" data-example="3" data-encoder="imagebind" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="3" data-encoder="imagebind" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">AudioCLIP</div>
                            <button class="mode-button" data-example="3" data-encoder="audioclip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="3" data-encoder="audioclip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">CLAP</div>
                            <button class="mode-button" data-example="3" data-encoder="clap" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="3" data-encoder="clap" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Wav2CLIP</div>
                            <button class="mode-button" data-example="3" data-encoder="wav2clip" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="3" data-encoder="wav2clip" data-mode="raw">Raw</button>
                        </div>
                        <div class="encoder-column">
                            <div class="encoder-label">Whisper</div>
                            <button class="mode-button" data-example="3" data-encoder="whisper" data-mode="projected">Projected</button>
                            <button class="mode-button raw" data-example="3" data-encoder="whisper" data-mode="raw">Raw</button>
                        </div>
                    </div>
                    
                    <div class="current-selection" id="selection-3">Select an encoder mode to view generated caption</div>
                    <div class="caption-display empty" id="caption-3">Click any button above to see the generated caption for this audiovisual example.</div>
                </div>
            </div>
        </section>

        <section id="dataset" class="section">
            <h2>AVE-2 Dataset</h2>
            <p>We introduce <strong>AudioVisual Event Evaluation (AVE-2)</strong>, a dataset of 580,147 three-second audiovisual clips with fine-grained alignment annotations. Unlike existing datasets that treat alignment as binary, AVE-2 provides detailed alignment scores across five dimensions, enabling systematic investigation of audio-visual correspondence quality.</p>

            <div class="stats">
                <div class="stat-item">
                    <span class="stat-number">580,147</span>
                    <div class="stat-label">Audio-Visual Clips</div>
                </div>
                <div class="stat-item">
                    <span class="stat-number">5</span>
                    <div class="stat-label">Alignment Dimensions</div>
                </div>
                <div class="stat-item">
                    <span class="stat-number">3</span>
                    <div class="stat-label">Seconds per Clip</div>
                </div>
                <div class="stat-item">
                    <span class="stat-number">10</span>
                    <div class="stat-label">Encoder Modes</div>
                </div>
            </div>
        </section>

        <section id="results" class="section">
            <h2>Key Findings</h2>
            <p>Our systematic investigation reveals a fundamental trade-off where excessive alignment with the visual manifold leads to improved retrieval capabilities, but simultaneously reduces the richness of acoustic and linguistic information necessary for quality text description generation.</p>

            <h3>Performance Trade-offs</h3>
            <p>Image-centric encoders (ImageBind, AudioCLIP, Wav2CLIP) demonstrate exceptional performance in cross-modal retrieval due to their inherent design for visual alignment, but this intensive alignment causes compression of unique linguistic information and reduces text generation quality. Text-centric encoders (CLAP, Whisper) maintain stronger linguistic authenticity and achieve better balance between retrieval and generation objectives.</p>

            <h3>Architectural Insights</h3>
            <p>The initial architectural type of the encoder determines how it responds to the alignment process. Encoders pre-trained with text supervision maintain stronger generative capabilities than those focused primarily on audiovisual alignment, highlighting the value of language exposure for generation tasks.</p>

            <h3>Pareto Frontier Discovery</h3>
            <p>We establish a clear Pareto frontier for cross-modal learning, providing guidelines for choosing between retrieval accuracy and generative richness based on application needs. This challenges the assumption that stronger cross-modal alignment necessarily benefits all multimodal tasks.</p>
        </section>

        <section id="code" class="section">
            <h2>Code and Resources</h2>
            <p>All code, data, and pre-trained models are made available to facilitate reproducibility and future research in audio-visual alignment.</p>

            <div class="btn-group">
                <a href="https://github.com/ali-vosoughi/SoundCLIP" class="btn btn-primary">GitHub Repository</a>
                <a href="https://arxiv.org/abs/2506.10416" class="btn">ArXiv Paper</a>
                <a href="#dataset" class="btn">Download Dataset</a>
            </div>
        </section>

        <section id="citation" class="section">
            <h2>Citation</h2>
            <p>If you use SoundCLIP or the AVE-2 dataset in your research, please cite our paper:</p>
            <div class="citation-box">
@article{vosoughi2025soundclip,
  title={Can Sound Replace Vision in LLaVA With Token Substitution?},
  author={Vosoughi, Ali and Bi, Jing and Liu, Pinxin and Tang, Yunlong and Xu, Chenliang},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2026}
}
            </div>
        </section>
    </div>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <h4>Authors</h4>
                    <p><a href="https://alivosoughi.com/">Ali Vosoughi</a></p>
                    <p><a href="https://jing.vision/">Jing Bi</a></p>
                    <p><a href="https://andypinxinliu.github.io/">Pinxin Liu</a></p>
                    <p><a href="https://yunlong10.github.io/">Yunlong Tang</a></p>
                    <p><a href="https://www.cs.rochester.edu/~cxu22/index.html">Chenliang Xu</a></p>
                </div>
                <div class="footer-section">
                    <h4>Resources</h4>
                    <p><a href="https://arxiv.org/abs/2506.10416">ArXiv Paper</a></p>
                    <p><a href="https://github.com/ali-vosoughi/SoundCLIP">GitHub Repository</a></p>
                    <p><a href="#dataset">AVE-2 Dataset</a></p>
                </div>
                <div class="footer-section">
                    <h4>Institution</h4>
                    <p>University of Rochester</p>
                    <p>Computer Science Department</p>
                    <p>Rochester, NY, USA</p>
                </div>
            </div>
            <p>&copy; 2025 University of Rochester Computer Science Department. All rights reserved.</p>
        </div>
    </footer>

    <script>
        // Sample caption data - replace with your actual captions
        const captionData = {
            "1": {
                "imagebind": {
                    "projected": "The image shows a person playing a guitar with sophisticated musical technique. The projected alignment captures the harmonic relationships between visual finger movements and the resulting acoustic patterns.",
                    "raw": "A person is playing a guitar. The raw audio encoding preserves the authentic acoustic characteristics without visual manifold compression."
                },
                "audioclip": {
                    "projected": "The image you've provided is of a person playing a guitar. We can hear the guitar's strings being plucked and the vibration of the strings being converted into sound by the guitar's body. The person is likely strumming or picking the strings, which produces the musical notes we can hear.",
                    "raw": "A person playing guitar with natural sound characteristics. Raw encoding maintains the full spectrum of acoustic information without projection constraints."
                },
                "clap": {
                    "projected": "Guitar performance with enhanced cross-modal alignment between visual and audio features, resulting in more consistent audio-visual correspondence.",
                    "raw": "Musical performance on acoustic guitar. Text-supervised encoding preserves linguistic richness while maintaining authentic audio representation."
                },
                "wav2clip": {
                    "projected": "Person playing guitar with optimized audio-visual synchronization. Projected features show improved retrieval performance through visual manifold alignment.",
                    "raw": "Guitar playing scene with preserved acoustic authenticity. Raw features maintain original audio characteristics without visual space compression."
                },
                "whisper": {
                    "projected": "Guitar music performance with balanced audio-visual integration. WhisperCLIP architecture maintains generative quality while achieving cross-modal alignment.",
                    "raw": "A person is playing an acoustic guitar, producing melodic sounds through string vibration. The raw encoding preserves the full linguistic and acoustic richness."
                }
            },
            "2": {
                "imagebind": {
                    "projected": "Natural outdoor environment with bird sounds and wind. Projected alignment enhances environmental audio-visual correspondence for improved scene understanding.",
                    "raw": "Birds chirping in a natural environment with wind sounds. Raw encoding maintains authentic environmental audio characteristics."
                },
                "audioclip": {
                    "projected": "Outdoor scene with natural bird calls and environmental sounds. Enhanced cross-modal alignment improves scene retrieval accuracy.",
                    "raw": "Natural environment with bird vocalizations and ambient sounds. Raw features preserve full acoustic detail without visual projection."
                },
                "clap": {
                    "projected": "Natural soundscape with bird songs and wind through trees. Text-supervised alignment maintains descriptive quality while improving retrieval.",
                    "raw": "Birds singing in their natural habitat with gentle wind sounds. Raw encoding preserves authentic environmental audio without compression."
                },
                "wav2clip": {
                    "projected": "Environmental scene with avian sounds and natural ambiance. Projected features show strong audio-visual scene correspondence.",
                    "raw": "Natural bird sounds in outdoor environment. Raw audio encoding maintains original environmental sound characteristics."
                },
                "whisper": {
                    "projected": "Natural environment with bird calls and wind sounds. Balanced encoding maintains both retrieval performance and generative quality.",
                    "raw": "Birds are singing in a peaceful natural environment with gentle wind sounds creating a serene atmosphere."
                }
            },
            "3": {
                "imagebind": {
                    "projected": "Urban street scene with traffic sounds and pedestrian activity. Projected alignment enhances understanding of complex urban audio-visual relationships.",
                    "raw": "City street with vehicles and pedestrian sounds. Raw encoding preserves the full complexity of urban acoustic environments."
                },
                "audioclip": {
                    "projected": "Busy urban environment with car engines and street noise. Cross-modal alignment improves scene recognition through audio-visual integration.",
                    "raw": "Urban street scene with traffic and ambient city sounds. Raw features maintain authentic urban acoustic characteristics."
                },
                "clap": {
                    "projected": "City street with vehicular traffic and urban activity. Text-supervised encoding balances descriptive accuracy with retrieval performance.",
                    "raw": "Urban environment with car sounds and street activity. Raw encoding preserves rich linguistic descriptions of city soundscapes."
                },
                "wav2clip": {
                    "projected": "Street scene with traffic flow and urban ambiance. Projected features demonstrate improved audio-visual scene correspondence.",
                    "raw": "City street with vehicle sounds and pedestrian activity. Raw audio maintains original urban sound complexity."
                },
                "whisper": {
                    "projected": "Urban street environment with traffic and city sounds. WhisperCLIP maintains generative quality while achieving effective alignment.",
                    "raw": "This is a busy city street with cars driving by and various urban sounds creating a typical metropolitan atmosphere."
                }
            }
        };

        // Handle encoder mode button clicks
        document.querySelectorAll('.mode-button').forEach(button => {
            button.addEventListener('click', function() {
                const example = this.dataset.example;
                const encoder = this.dataset.encoder;
                const mode = this.dataset.mode;
                
                // Remove active class from all buttons in this example
                document.querySelectorAll(`[data-example="${example}"]`).forEach(btn => {
                    btn.classList.remove('active');
                });
                
                // Add active class to clicked button
                this.classList.add('active');
                
                // Update selection display
                const selectionEl = document.getElementById(`selection-${example}`);
                selectionEl.textContent = `${encoder.toUpperCase()} - ${mode.charAt(0).toUpperCase() + mode.slice(1)} Mode`;
                
                // Update caption
                const captionEl = document.getElementById(`caption-${example}`);
                const caption = captionData[example][encoder][mode];
                captionEl.textContent = caption;
                captionEl.classList.remove('empty');
            });
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>
